{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning Course - Assignment 2\n",
    "Group Members: Iulia-Elena Teodorescu, Adnan Al Medawer, Konstantinos Tantoulas, Likhith Bedara Agrahara Venkateshamurthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, lr,state_size, n_actions, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x) \n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, n_actions, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the target network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, n_actions, seed):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_policy = DQN(LR, state_size, n_actions, seed)#.to(device)\n",
    "        self.qnetwork_target = DQN(LR, state_size, n_actions, seed)#.to(device)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(n_actions, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience and perform learning.\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            self.learn(self.memory.sample(), GAMMA)\n",
    "\n",
    "\n",
    "    def select_action(self, state, eps=0.):\n",
    "        \"\"\"Choose an action based on epsilon-greedy policy.\"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_policy(state)\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() > eps:\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        return action\n",
    "\n",
    "                   \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update Q-network parameters using a batch of experience tuples.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Compute target Q values for the current state\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = self.qnetwork_target(next_states).max(1, keepdim=True)[0]\n",
    "            Q_targets = rewards + gamma * Q_targets_next * (1 - dones)\n",
    "        \n",
    "        # Compute the predicted Q values for the chosen actions\n",
    "        Q_expected = self.qnetwork_policy(states).gather(1, actions)\n",
    "\n",
    "        # Update policy Q-network\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.qnetwork_policy.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.qnetwork_policy.optimizer.step()\n",
    "\n",
    "        # Update target Q-network\n",
    "        self.update_network(self.qnetwork_policy, self.qnetwork_target, TAU)\n",
    "\n",
    "    def update_network(self, local_model, target_model, tau):\n",
    "        \"\"\" Update target model parameters.\n",
    "        θ_target = τ*θ_policy + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, n_episodes=2000, max_t=1000, eps=1.0, eps_min=0.01, eps_decay=0.995):\n",
    "    \"\"\"Train the agent using Deep Q-Learning.\"\"\"\n",
    "    rewards = []                          # List to store total rewards for each episode\n",
    "    epsilon_values = []                   # List to store epsilon values for each episode\n",
    "    rewards_window = deque(maxlen=100)    # Last 100 rewards for average reward\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, info = env.reset()         # Reset environment and get initial state\n",
    "        score = 0\n",
    "        for t in range(max_t):           \n",
    "            action = agent.select_action(state, eps)  # Get action based on epsilon-greedy policy\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.step(state, action, reward, next_state, done)  # Store experience and learn\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Log the total reward and epsilon for this episode\n",
    "        rewards.append(score)\n",
    "        epsilon_values.append(eps)\n",
    "        rewards_window.append(score)  # Add current episode score to the window\n",
    "        \n",
    "        if eps > eps_min:                 # Decay epsilon\n",
    "            eps *= eps_decay\n",
    "        else:\n",
    "            eps = eps_min\n",
    "        \n",
    "        # Print the episode information\n",
    "        print(f'\\rEpisode {i_episode}/{n_episodes}\\t Total Reward: {score:.2f}\\tEpsilon: {eps:.2f}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}/{n_episodes}\\tAverage Reward: {np.mean(rewards_window):.2f}')\n",
    "        if np.mean(rewards_window) >= 200.0:  # Check if the environment is solved\n",
    "            print(f'\\nEnvironment solved in {i_episode-100} episodes!\\tAverage Reward: {np.mean(rewards_window):.2f}\\tEpsilon: {eps:.3f}')\n",
    "            torch.save(agent.qnetwork_policy.state_dict(), 'checkpoint_lunar.pth') \n",
    "            break\n",
    "    \n",
    "    return rewards, epsilon_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, n_episodes=100, max_t=1000, eps=0.):\n",
    "    \"\"\"Test the agent after training.\"\"\"\n",
    "    rewards = []  # List to store total rewards for each episode\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, info = env.reset()  # Reset environment and get initial state\n",
    "        score = 0\n",
    "        for t in range(max_t):     # Loop over each time step in the episode\n",
    "            action = agent.select_action(state, eps)  # Get action based on epsilon-greedy policy\n",
    "            next_state, reward, done, info, _ = env.step(action)  # Step in the environment\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Log the total reward for this episode\n",
    "        rewards.append(score)\n",
    "        print(f'\\rTest Episode {i_episode}/{n_episodes}\\tTotal Reward: {score:.2f}', end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f'\\rTest Episode {i_episode}/{n_episodes}\\tTotal Reward: {score:.2f}')\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = gym.make('LunarLander-v3', render_mode='human')\n",
    "agent = Agent(state_size=env.observation_space.shape[0], n_actions=env.action_space.n, seed=0)\n",
    "\n",
    "# Train the agent and log the total rewards and epsilon\n",
    "train_rewards, epsilon_values = train(agent, env, n_episodes=2000, eps=1.0, eps_min=0.01, eps_decay=0.995)\n",
    "\n",
    "# Test the agent after training\n",
    "test_rewards = test(agent, env, n_episodes=100, max_t=1000, eps=0.)\n",
    "\n",
    "# Plot the training rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(train_rewards)), train_rewards, label='Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Rewards')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the testing rewards\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(len(test_rewards)), test_rewards, label='Test Rewards', color='green')\n",
    "plt.xlabel('Test Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Testing Rewards')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "env.close()  # Close the environment after testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
