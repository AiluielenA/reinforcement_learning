{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:51:18.708887Z",
     "start_time": "2024-11-27T14:51:18.698917Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "from time import strftime\n",
    "import csv\n",
    "from environment_class import Environment\n",
    "import robot_class as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d60e7f583ab3507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:51:18.787717Z",
     "start_time": "2024-11-27T14:51:18.776178Z"
    }
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self):\n",
    "        # Create a unique log directory\n",
    "        self.log_dir = f\"log_{strftime('%Y_%m_%d-%H_%M_%S')}\"\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        # Paths for logging files\n",
    "        self.rewards_log_path = os.path.join(self.log_dir, \"rewards.csv\")\n",
    "        self.transitions_log_path = os.path.join(self.log_dir, \"transitions.csv\")\n",
    "        self.evaluation_log_path = os.path.join(self.log_dir, \"evaluation_rewards.csv\")\n",
    "        self.model_save_path = os.path.join(self.log_dir, \"trained_model.keras\")\n",
    "\n",
    "        # Initialize the CSV log files\n",
    "        with open(self.rewards_log_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"episode\", \"reward\"])\n",
    "\n",
    "        with open(self.transitions_log_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"episode\", \"state\", \"action\", \"next-state\", \"reward\", \"done\", \"truncated\"])\n",
    "\n",
    "        with open(self.evaluation_log_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"episode\", \"reward\"])\n",
    "\n",
    "    def log_reward(self, episode, reward):\n",
    "        with open(self.rewards_log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([episode, reward])\n",
    "            \n",
    "    def log_eval_reward(self, episode, reward):\n",
    "        with open(self.rewards_log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([episode, reward])\n",
    "\n",
    "    def log_transition(self, episode, state, action, next_state, reward, done, truncated):\n",
    "        with open(self.transitions_log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([episode, state.tolist(), action, next_state.tolist(), reward, done, truncated])\n",
    "\n",
    "    def save_model(self, model):\n",
    "        model.save(self.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef723a1576aafb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:51:18.866955Z",
     "start_time": "2024-11-27T14:51:18.844715Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExponentialDecay:\n",
    "    def __init__(self, initial_epsilon, decay_rate, min_epsilon):\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "        self.current_epsilon = initial_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def get_value(self):\n",
    "        return max(self.current_epsilon, self.min_epsilon)\n",
    "\n",
    "    def update(self):\n",
    "        if self.current_epsilon > self.min_epsilon:\n",
    "            self.current_epsilon *= self.decay_rate\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "            self,\n",
    "            main_model,\n",
    "            target_model=None,\n",
    "            epsilon=None,\n",
    "            gamma=None,\n",
    "            action_space=None,\n",
    "            max_buffer=50000\n",
    "    ) -> None:\n",
    "        self.action_space = copy.deepcopy(action_space)\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.buffer_replay = deque(maxlen=max_buffer)\n",
    "        self.main_model = main_model\n",
    "        self.target_model = target_model\n",
    "        self.loss = tf.keras.losses.Huber()\n",
    "        self.optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def select_action(self, state, num_robots):\n",
    "        if random.random() < self.epsilon.get_value():\n",
    "            self.epsilon.update()\n",
    "            # Return a random action for each robot\n",
    "            return [random.choice(self.action_space) for _ in range(num_robots)]\n",
    "        else:\n",
    "            # Return the best action for each robot\n",
    "            return self.select_best_action(state, num_robots)\n",
    "\n",
    "\n",
    "    def select_best_action(self, state):\n",
    "        q_values = self.main_model.predict(state[np.newaxis], verbose=0)\n",
    "        print(q_values)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def store_transition(self, state, action, next_state, reward, done, truncated):\n",
    "        self.buffer_replay.append((state, action, next_state, reward, done, truncated))\n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer_replay), size=batch_size)\n",
    "        return [self.buffer_replay[idx] for idx in indices]\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.buffer_replay) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = self.sample_minibatch(batch_size)\n",
    "        states, actions, next_states, rewards, dones, truncates = map(np.array, zip(*minibatch))\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_q_values = self.target_model(next_states)\n",
    "            q_targets = np.max(next_q_values, axis=-1)\n",
    "            q_targets = rewards + (1 - (dones | truncates)) * tf.squeeze(q_targets) * self.gamma\n",
    "            current_q_values = self.main_model(states)\n",
    "            indices = tf.stack([tf.range(len(actions)), actions], axis=1)\n",
    "            q_values_selected = tf.gather_nd(current_q_values, indices)\n",
    "            loss = self.loss(q_values_selected, q_targets)\n",
    "            \n",
    "        value_grads = tape.gradient(loss, self.main_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(value_grads, self.main_model.trainable_variables))\n",
    "\n",
    "    def update_target_model(self, weights=None):\n",
    "        if weights is not None:\n",
    "            self.target_model.set_weights(weights)\n",
    "        else:\n",
    "            self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "\n",
    "class CreateNetwork(tf.keras.Model):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CreateNetwork, self).__init__()\n",
    "        self.dens1 = layers.Dense(64, activation=tf.keras.activations.leaky_relu)\n",
    "        self.dens2 = layers.Dense(64, activation='relu')\n",
    "        self.dens3 = layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dens1(state)\n",
    "        x = self.dens2(x)\n",
    "        x = self.dens3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Helper function to flatten the state space for the neural network input\n",
    "def flatten_state(state, grid_size):\n",
    "    return np.concatenate([\n",
    "        state[\"robots\"].flatten(),\n",
    "        state[\"package_positions\"].flatten(),\n",
    "        state[\"target_positions\"].flatten()\n",
    "    ]).astype(np.float32) / grid_size\n",
    "\n",
    "def train_agent(env, agent, episodes, update_target_freq, logger=None):\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "        steps = 0\n",
    "\n",
    "        while not terminated  and steps < 1000:\n",
    "            steps +=1\n",
    "            action = agent.select_action(state,2)\n",
    "            print(action)\n",
    "            \n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            agent.store_transition(state, action, next_state, reward, done, truncated)\n",
    "            agent.train(batch_size=32)\n",
    "            terminated = done or truncated\n",
    "                \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        # Log rewards\n",
    "        if logger:\n",
    "            logger.log_reward(episode, total_reward)\n",
    "            \n",
    "        # Update target network\n",
    "        if episode % update_target_freq == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon.get_value():.2f}, Steps: {steps}\")\n",
    "    print(f'AVG reward: {np.mean(rewards)}')\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, episodes, logger=None):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "        steps = 0\n",
    "        while not terminated:\n",
    "            action = agent.select_best_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            terminated = done or truncated\n",
    "                \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        # Log rewards\n",
    "        if logger:\n",
    "            logger.log_eval_reward(episode, total_reward)\n",
    "            \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward:.2f}, Steps: {steps}\")\n",
    "    env.close()\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def plot_rewards(root_folder, rewards, title, save_fig=False):\n",
    "    plt.plot(rewards, label='Total Reward per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(title)\n",
    "    if save_fig:\n",
    "        plt.savefig(root_folder + \"/\" + title + '_' + strftime(\"%Y_%m_%d_%H_%M_%S\") +'_' + '.png')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe4a10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('warehouse-robot', render_mode='human')\n",
    "# env = Environment(grid_rows=7, grid_cols=7, num_robots=2, num_packages=2, num_targets=2, num_obstacles=4, num_charger=2)\n",
    "\n",
    "# Define state and action sizes\n",
    "num_robots = env.unwrapped.num_robots\n",
    "num_packages = env.unwrapped.num_packages\n",
    "num_targets = env.unwrapped.num_targets\n",
    "grid_size = env.unwrapped.grid_rows * env.unwrapped.grid_cols\n",
    "state_dim = (num_robots + num_packages + num_targets) * 2  # Positions for robots, packages, and targets\n",
    "action_dim = len(r.RobotAction) * num_robots  # Actions for all robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "598101c7ae3022fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:53:17.964766Z",
     "start_time": "2024-11-27T14:51:18.953396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 6]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "10 is not a valid RobotAction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger()\n\u001b[1;32m     15\u001b[0m training_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 16\u001b[0m training_rewards \u001b[38;5;241m=\u001b[39m train_agent(env\u001b[38;5;241m=\u001b[39menv, agent\u001b[38;5;241m=\u001b[39magent, episodes\u001b[38;5;241m=\u001b[39mtraining_episodes,update_target_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m ,logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m     17\u001b[0m plot_rewards(logger\u001b[38;5;241m.\u001b[39mlog_dir, training_rewards, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[53], line 118\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, episodes, update_target_freq, logger)\u001b[0m\n\u001b[1;32m    115\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m--> 118\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    119\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, next_state, reward, done, truncated)\n\u001b[1;32m    120\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/2024JU/reinforcement learning/assign. +git/reinforcement_learning/project/environment_class.py:118\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, robot_actions, fps)\u001b[0m\n\u001b[1;32m    115\u001b[0m robots_sorted_by_energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobots, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m r: r\u001b[38;5;241m.\u001b[39menergy)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, robot \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(robots_sorted_by_energy):\n\u001b[0;32m--> 118\u001b[0m     action \u001b[38;5;241m=\u001b[39m RobotAction(robot_actions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobots\u001b[38;5;241m.\u001b[39mindex(robot)])\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Consume energy for any action\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     robot\u001b[38;5;241m.\u001b[39menergy \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/enum.py:712\u001b[0m, in \u001b[0;36mEnumType.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start, boundary)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    715\u001b[0m         value,\n\u001b[1;32m    716\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m         boundary\u001b[38;5;241m=\u001b[39mboundary,\n\u001b[1;32m    722\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/enum.py:1135\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m   1133\u001b[0m ve_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (value, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m))\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve_exc\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1138\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m._missing_: returned \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m instead of None or a valid member\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1139\u001b[0m             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, result)\n\u001b[1;32m   1140\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: 10 is not a valid RobotAction"
     ]
    }
   ],
   "source": [
    "# Training cell\n",
    "main_network = CreateNetwork(action_dim)\n",
    "target_network = CreateNetwork(action_dim)\n",
    "epsilon_schedule = ExponentialDecay(initial_epsilon=1.0, decay_rate=0.995, min_epsilon=0.01)\n",
    "\n",
    "agent = DQN(\n",
    "    main_model=main_network,\n",
    "    target_model=target_network,\n",
    "    epsilon=epsilon_schedule,\n",
    "    gamma=0.99,\n",
    "    action_space=list(range(action_dim))\n",
    ")\n",
    "logger = Logger()\n",
    "\n",
    "training_episodes = 1000\n",
    "training_rewards = train_agent(env=env, agent=agent, episodes=training_episodes,update_target_freq=20 ,logger=logger)\n",
    "plot_rewards(logger.log_dir, training_rewards, \"Training rewards\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba479d02175773d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:53:26.610020Z",
     "start_time": "2024-11-27T14:53:18.099528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation cell\n",
    "evaluation_episodes = 100\n",
    "evaluate_rewards = evaluate_agent(env=env, agent=agent, episodes=evaluation_episodes, logger=logger)\n",
    "plot_rewards(logger.log_dir,evaluate_rewards, \"Evaluation rewards\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
